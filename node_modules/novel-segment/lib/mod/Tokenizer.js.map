{"version":3,"file":"Tokenizer.js","sourceRoot":"","sources":["Tokenizer.ts"],"names":[],"mappings":"AAAA;;;;GAIG;AAEH,YAAY,CAAC;;;;;;;;;AAEb,qDAA2C;AAE3C,+BAA2F;AAc3F;IAAA,IAAsB,mBAAmB;IADzC,aAAa;IACb,MAAsB,mBAAoB,SAAQ,gBAAU;QAA5D;;YAGiB,SAAI,GAAG,WAAW,CAAC;QAwFpC,CAAC;QApFO,IAAI,CAAC,OAAgB,EAAE,GAAG,IAAI;YAEpC,KAAK,CAAC,IAAI,CAAC,OAAO,EAAE,GAAG,IAAI,CAAC,CAAC;YAE7B,OAAO,IAAI,CAAC;QACb,CAAC;QAEM,MAAM,CAAC,IAAI,CAAsD,OAAgB,EAAE,GAAG,IAAI;YAEhG,aAAa;YACb,OAAO,KAAK,CAAC,IAAI,CAAI,OAAO,EAAE,GAAG,IAAI,CAAC,CAAC;QACxC,CAAC;QAED;;;WAGG;QACO,WAAW,CAAuC,KAAU,EAAE,EAAkC;YAEzG,qCAAqC;YAErC,EAAE,GAAG,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAEnB,IAAI,GAAG,GAAG,EAAE,CAAC;YACb,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,IAAI,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAC1C;gBACC,IAAI,OAAO,IAAI,CAAC,CAAC,IAAI,QAAQ,EAC7B;oBACC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;iBACf;qBAED;oBACC,IAAI,SAAS,GAAG,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;oBAE3B,IAAI,SAAS,IAAI,IAAI,EACrB;wBACC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;qBACf;yBAED;wBACC,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;qBAC5B;iBACD;aACD;YAED,OAAO,GAAG,CAAC;QACZ,CAAC;QAED;;;WAGG;QACO,YAAY,CAAuC,KAAU,EAAE,EAAkC;YAE1G,qCAAqC;YAErC,EAAE,GAAG,EAAE,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAEnB,IAAI,GAAG,GAAG,EAAE,CAAC;YACb,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,IAAI,EAAE,IAAI,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAC1C;gBACC,IAAI,IAAI,CAAC,CAAC,EACV;oBACC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;iBACf;qBAED;oBACC,wCAAwC;oBACxC,IAAI,SAAS,GAAG,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;oBAE3B,IAAI,OAAO,SAAS,IAAI,WAAW,IAAI,SAAS,KAAK,IAAI,EACzD;wBACC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;qBACf;yBAED;wBACC,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC;qBAC5B;iBAED;aACD;YAED,OAAO,GAAG,CAAC;QACZ,CAAC;KACD,CAAA;IAzFuB,wBAAI,GAAG,WAAW,CAAC;IAFrB,mBAAmB;QAFxC,0BAAQ;QACT,aAAa;OACS,mBAAmB,CA2FxC;IAAD,0BAAC;KAAA;AA3FqB,kDAAmB;AA6FzC;;GAEG;AACH,MAAa,SAAU,SAAQ,aAAO;IAAtC;;QAEC,SAAI,GAAG,WAAW,CAAC;IAsCpB,CAAC;IApCA;;;;;;OAMG;IACH,KAAK,CAAC,IAAY,EAAE,IAAqB,EAAE,GAAG,IAAI;QAEjD,IAAI,IAAI,CAAC,MAAM,GAAG,CAAC,EACnB;YACC,MAAM,KAAK,CAAC,sBAAsB,CAAC,CAAC;SACpC;aAED;YACC,IAAI,GAAG,GAAY,CAAC,EAAE,CAAC,EAAE,IAAI,EAAE,CAAC,CAAC;YAEjC,OAAO,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE,GAAG,EAAE,IAAI,EAAE,GAAG,IAAI,CAAC,CAAC;YAEnD;;;;;;;;;;;;;;cAcE;SACF;IACF,CAAC;CACD;AAxCD,8BAwCC;AAED,kBAAe,SAAS,CAAC","sourcesContent":["/**\n * 分词模块管理器\n *\n * @author 老雷<leizongmin@gmail.com>\n */\n\n'use strict';\n\nimport { autobind } from 'core-decorators';\nimport { Segment, IWord } from '../Segment';\nimport { IModuleStatic, ISubSModule, SModule, SubSModule, ISubSModuleCreate } from './mod';\n\nexport type ISubTokenizer = ISubSModule & {\n\ttype: 'tokenizer',\n\tsplit(words: IWord[], ...argv): IWord[],\n}\n\nexport type ISubTokenizerCreate<T extends SubSModuleTokenizer, R extends SubSModuleTokenizer = SubSModuleTokenizer> = {\n\t(...argv: Parameters<T[\"init\"]>): T & R,\n\t(segment: Segment, ...argv): T & R,\n};\n\n@autobind\n// @ts-ignore\nexport abstract class SubSModuleTokenizer extends SubSModule implements ISubTokenizer\n{\n\tpublic static readonly type = 'tokenizer';\n\tpublic readonly type = 'tokenizer';\n\n\tpublic abstract split(words: IWord[], ...argv): IWord[]\n\n\tpublic init(segment: Segment, ...argv)\n\t{\n\t\tsuper.init(segment, ...argv);\n\n\t\treturn this;\n\t}\n\n\tpublic static init<T extends SubSModuleTokenizer = SubSModuleTokenizer>(segment: Segment, ...argv): T\n\t{\n\t\t// @ts-ignore\n\t\treturn super.init<T>(segment, ...argv);\n\t}\n\n\t/**\n\t * 仅对未识别的词进行匹配\n\t * 不包含 p 為 0\n\t */\n\tprotected _splitUnset<T extends IWord, U extends IWord = T>(words: T[], fn: (text: string, ...argv) => U[]): U[]\n\t{\n\t\t//const POSTAG = this.segment.POSTAG;\n\n\t\tfn = fn.bind(this);\n\n\t\tlet ret = [];\n\t\tfor (let i = 0, word; word = words[i]; i++)\n\t\t{\n\t\t\tif (typeof word.p == 'number')\n\t\t\t{\n\t\t\t\tret.push(word);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tlet words_new = fn(word.w);\n\n\t\t\t\tif (words_new == null)\n\t\t\t\t{\n\t\t\t\t\tret.push(word);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tret = ret.concat(words_new);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn ret;\n\t}\n\n\t/**\n\t * 仅对未识别的词进行匹配\n\t * 包含已存在 但 p 為 0\n\t */\n\tprotected _splitUnknow<T extends IWord, U extends IWord = T>(words: T[], fn: (text: string, ...argv) => U[]): U[]\n\t{\n\t\t//const POSTAG = this.segment.POSTAG;\n\n\t\tfn = fn.bind(this);\n\n\t\tlet ret = [];\n\t\tfor (let i = 0, word; word = words[i]; i++)\n\t\t{\n\t\t\tif (word.p)\n\t\t\t{\n\t\t\t\tret.push(word);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t//let words_new = fn.call(this, word.w);\n\t\t\t\tlet words_new = fn(word.w);\n\n\t\t\t\tif (typeof words_new == 'undefined' || words_new === null)\n\t\t\t\t{\n\t\t\t\t\tret.push(word);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tret = ret.concat(words_new);\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\n\t\treturn ret;\n\t}\n}\n\n/**\n * 分词模块管理器\n */\nexport class Tokenizer extends SModule\n{\n\ttype = 'tokenizer';\n\n\t/**\n\t * 对一段文本进行分词\n\t *\n\t * @param {string} text 文本\n\t * @param {array} modules 分词模块数组\n\t * @return {array}\n\t */\n\tsplit(text: string, mods: ISubTokenizer[], ...argv)\n\t{\n\t\tif (mods.length < 1)\n\t\t{\n\t\t\tthrow Error('No tokenizer module!');\n\t\t}\n\t\telse\n\t\t{\n\t\t\tlet ret: IWord[] = [{ w: text }];\n\n\t\t\treturn this._doMethod('split', ret, mods, ...argv);\n\n\t\t\t/*\n\t\t\t// 按顺序分别调用各个module来进行分词 ： 各个module仅对没有识别类型的单词进行分词\n\t\t\tmods.forEach(function (mod)\n\t\t\t{\n\t\t\t\t// @ts-ignore\n\t\t\t\tif (typeof mod._cache == 'function')\n\t\t\t\t{\n\t\t\t\t\t// @ts-ignore\n\t\t\t\t\tmod._cache();\n\t\t\t\t}\n\n\t\t\t\tret = mod.split(ret, ...argv);\n\t\t\t});\n\t\t\treturn ret;\n\t\t\t*/\n\t\t}\n\t}\n}\n\nexport default Tokenizer;\n"]}